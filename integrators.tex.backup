\pdfminorversion=4

\documentclass[preprint,12pt]{elsarticle}
\usepackage[utf8]{inputenc}

%packages
\usepackage[margin=1in]{geometry}

\usepackage[hyphens]{url}
\biboptions{sort&compress, square, comma}
\usepackage[breaklinks=true, linkcolor=blue, citecolor=blue, colorlinks=true]{hyperref}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[version=3]{mhchem} % Formula subscripts using \ce{}, e.g., \ce{H2SO4}
\usepackage{latexsym,amsmath,amssymb}

\usepackage{mathtools}

\usepackage{siunitx}
\sisetup{group-separator={,},
     detect-all,
     binary-units,
     list-units = single,
     range-units = single,
     tophrase = --, 
     per-mode = symbol-or-fraction,
     separate-uncertainty = true,
     list-final-separator = {, and }
%    scientific-notation = fixed
}
\DeclareSIUnit\atm{atm}

\graphicspath{{./figures/}}

\journal{36th International Symposium on Combustion}

\begin{document}
\begin{frontmatter}

\title{An Investigation into GPU accelerated Chemical Kinetic Integration}

\author[uconn]{Nicholas~J.\ Curtis}
\author[osu]{Kyle~E.\ Niemeyer}
\author[uconn]{Chih-Jen Sung\corref{cor1}}
\ead{cjsung@engr.uconn.edu}

% addresses
\address[uconn]{Department of Mechanical Engineering\\
  University of Connecticut, Storrs, CT, 06269, USA}
\address[osu]{School of Mechanical, Industrial, and Manufacturing Engineering\\
  Oregon State University, Corvallis, OR 97331, USA}
  
\cortext[cor1]{Corresponding author}

\begin{abstract}
TODO
\end{abstract}

\begin{keyword}
 Chemical Kinetics \sep Stiff Chemistry Integration \sep SIMD \sep GPU
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:Intro}

The need for detailed, accurate chemical kinetic models for predictive reacting-flow simulations has driven the developement of large hydrocarbon oxidation models for transport and energy relevant fuels.
At the same time, the growing understanding of the hydrocarbon oxidation process has resulted in orders of magnitude growth in mechanism size and complexity.  
For instance, a recently developed 2-methylalkane model, relevant for jet and diesel fuel surrogates, consists of over 7000 species and 30000 reactions \cite{Sarathy:2011kx} while a recent detailed gasoline surrogate mechanism contains over 1500 species and 6000 reactions \cite{Mehl:2011jn}.
Further, large hydrocarbon fuels tend to exhibit high levels of chemical stiffness \cite{Lu:2009gh}; the solution cost of which scales at best quadratically, and at worst cubically with the number of species in a mechanism \cite{Lu:2009gh}.

Consequently, a number of techniques have been developed to accelerate chemical kinetic integration.
These can be roughly categorized as: skeletal mechanism reduction and removal of unimportant species and reactions \cite{Lu:2005,Lu:2006bb,Lu:2008bi,Pepiot-Desjardins:2008,Niemeyer:2010bt,Niemeyer:2014,Curtis:2015aa,rabitz_sa,turanyi_sa_1,turanyi_sa_2,vajda_pca,valorani_csp2,valorani_csp}, time-scale analysis 
\cite{qssa,pe_approx1,pe_approx2} and dimensional reduction \cite{Lam:1993ub,Lam:1988wc,Lam:1994ws,Lu:2001ve,ildm}, and tabulation and interpolation of expensive terms \cite{Pope:1997wu,prism,Christo1996}.
In addition to these cost-reduction methods, significant work has been directed towards improvements of the integration algorithmns themselves.

Typically to efficiently solve the highly stiff set of governing equations associated with transport and fuel relevant chemical kinetics mechanisms, high-order implicit integration techniques are used.
These methods require repeated evaluation and factorization of the chemical kinetic Jacobian matrix in order to solve the associated non-linear algebraic equations during iteration, the cost of which scale quadratically and cubically with the number of species in a mechanism.
However, significant cost savings can be realized in Jacobian evaluation through the use of an analytic formulation, rather than the typical evaluation via a finite difference approximation.
This approach eliminates the numerous right-hand side function evaluations, and the cost of Jacobian evaluation drops to a linear dependence on the number of species in the mechanism \cite{Lu:2009gh}.
Several analytical Jacobian matrix codes have been developed \cite{Safta:2011vn,Youssefi:2011tm,Bisetti:2012jw,Perini:2012gy,Dijkmans:2014bb}, but the recently released \texttt{pyJac} \cite{Niemeyer:2015im,Niemeyer:2015ws} software is currently the only open-source analytical chemical kinetic Jacobian tool capable of both generating code for new SIMD processor types, as well as handling newer pressure dependence formulations (e.g. pressure-log or Chebyshev rate formulations).

Another research thrust has been aimed at evaluation of new integration algorithmns, specifically targeted at exploiting the power of high-performance hardware accelerators such as the Graphics Processing Unit (GPU) and other similar single-instruction multiple-data devices.
Central processing units (CPU) clock speeds have increased regularly--commonly known as Moore's Law--over the past few decades, however power consumption and heat dissapation issues have slowed this trend recently.
While multiple-core parallelism has increased CPU performance somewhat, recently SIMD processors have gained popularity as a low-cost, low-power and massively parallel high-performance computing alternative.
GPUs were originally developed for graphics/video processing and consist of hundreds to thousands of separate cores, compared to the tens of cores found on typical CPUs.

In the SIMD model, the total number of threads is broken into a grid of thread-blocks; each thread-block is assigned to a streaming multiprocessor (there are several on a SIMD device) and share a small memory cache, register bank and per block memory, known as shared memory.
During execution, thread-blocks are broken into groups of up to 32 threads--known as a warp.
Accelerations are achieved by applying the same operation across as a warp to a batch of data, when different threads in the warp must execute different instructions, as in a branching if/then statement, the different operations must be performed serially and a slowdown occurs -- an issue known as thread divergence.
Additionally, SIMD processors typically feature very small (relative to modern CPUs) per-thread cache and register bank sizes.
In order to avoid prohibitively slow memory access, accesses to the device memory should coalesced, that is all threads in a block should access memory locations that can be grouped together into one memory transaction.
Further, the shared-memory can be utilized as a manually managed per-block cache and steps can be attemped to maximize register usage and cache-hit rates for maximum memory access speed.
These two factors, thread-divergence and memory access patterns are two key perfomance parameters that determine the suitibility of an an algorithmn for SIMD parallelization.

A number of studies in recent years have explored the use high-performance SIMD devices for acceleration of turbulent reacting flow simulations.
Spafford et al. \cite{Spafford:2010aa} investigated the use of GPUs to accelerate S3D \cite{CHEN:2009s3d} a turbulent combustion direct numerical simulation code; they found around an order of magnitude speedup for evaluating the species production rates on the GPU.
Shi et al. \cite{Shi:2011aa} used the GPU for evaluation of species rates and Jacobian factorization and similarly large speedups when evaulating the rates or factorizing Jacobians on the GPU.
Niemeyer et al. \cite{Niemeyer:2011aa} implemented an explicit fourth-order Runge-Kutta integrator for the GPU, and found a speedup of nearly two orders of magnitude with a non-stiff hydrogen mechanism.
Further, the level of thread-divergence due to differing integrator time step sizes was investigated and found to have a large negative impact on overall performance when the initial conditions for the ODEs in a thread-block were very different.
Shi et al. \cite{Shi:2012aa} implemented a stabilized explicit solver, and paired it with a CPU based implicit solver which handled integration of the stiffest chemistry cells in a three-dimensional homogenous charge compression ignition engine simulation, domonstrating a 2-3$\times$ overall speedup for the full simulation.
Le et al. \cite{Le2013596} implemented a GPU version of two high-order shock-capturing reacting flow codes, and found a 30-50$\times$ speedup over the baseline 
Stone et al. \cite{Stone:2013aa} implemented the implicit VODE \cite{brown1989vode} solver for the GPU and achieved an order of magnitude speed-up over the baseline CPU version.
Additionally, it was found that the implicit VODE algorithm was highly susciptible to thread divergence, as expected from its relatively complicated (compared to an explicit integration scheme) program flow and if/then branching.
Further, for reasonable numbers of independent ODEs (e.g. $\mathcal{10^3}$) it was more efficient for each GPU thread solve one independent chemical kinetic ODE, rather than having a block of GPU threads cooperate to solve a single ODE \cite{Stone:2013aa}.
Niemeyer et al. \cite{Niemeyer:2014aa} found over an order of magnitude speedup for an implementation of a stabilized explicit second corder Runge-Kutta-Chebyshev algorithmn over a CPU implementation of VODE for moderately stiff chemical kinetics.
Sewerin et al. \cite{Sewerin20151375} implemented a 3-stage/5th order implicit Runge--Kutta method \cite{hairer1996solving} on a one-block per ODE basis, and found a maximum 5$\times$ speedup.

In this work we will investigate GPU implementations of several semi-implicit and implicit integration techniques, as compared to their CPU counterparts and a baseline CPU VODE implementation \cite{Hindmarsh:2005hg}.
Several previous works \cite{Stone:2013aa,Bisetti:2012jw,Niemeyer:2014aa,Perini20141180,McNenly2015581} have suggested so called matrix-free methods---which do not require direct factorization of the Jacobian, but instead use an iterative process to approximate the action of the factorized Jacobian on a vector---as potential improvements to the linear-system solver.
In particular, semi-implicit exponential integration methods have been suggested as a good fit for the SIMD parallelism model \cite{Stone:2013aa,Bisetti:2012jw,Niemeyer:2014aa} due to their relatively comparable performance with high-order implicit methods and the lesser expected thread-divergence performance impact as compared to fully implicit method.
Further, the 3-stage/5th order implicit Runge--Kutta algorithm \cite{hairer1996solving} investigated by Sewerin et al. \cite{Sewerin20151375} will be studied on a one-thread per ODE bases, in particular to determine the impact of increasing chemical stiffness on the algorithm.

\section{Methodology}
\label{sec:Method}

\subsection{Integration techniques}

Several integration methods were implemented/utilized for this work.
The aforementioned \texttt{pyJac} code \cite{Niemeyer:2015im} was used to provide both rate and analytical Jacobian subroutines for both CPU and GPU based algorithms.
For valididation and performance assessments of \texttt{pyJac} the reader is directed to our previous work \cite{Niemeyer:2015ws}.

To provide a good assessment of the baseline performance of a CPU based high-order implicit integration technique the CVODEs package \cite{Hindmarsh:2005hg} was utilized.
In addition, the 3-stage/5th order implicit Runge--Kutta algorithm \cite{hairer1996solving}, termed the Radau-IIA algorithm in this work, was implemented for the CPU.
Version 11.1.3 of the Intel MKL library was used to accelerate BLAS/LAPACK operations.
The commonly used fourth-order exponential Rosenbrock-like method \texttt{exp4} of Hochbruck et al. \cite{Hochbruck:1998} as well as their newer fourth-order exponential Rosenbrock method \cite{Hockbruck:2009}, termed \texttt{exprb43} in this work, were first implemented for the CPU for direct comparison to the high-order implicit techniques.
As suggested by Bissetti et al. \cite{Bisetti:2012jw} the method of rational approximants \cite{gallopoulos:1992} paired with the Carath\'edothy--Fej\'er method \cite{trefethen:2006} was used to compute the the approximation of the matrix-exponetial's action on a vector.
Unlike Bissetti et al., a custom routine based on \cite{stewart:1998} was written in order to find the LU decomposition of the Hessenberg matrix resulting from the Arnoldi process.

\subsection{Validation}

\pagebreak

\bibliography{refs}
\bibliographystyle{elsarticle-num}

\end{document}
